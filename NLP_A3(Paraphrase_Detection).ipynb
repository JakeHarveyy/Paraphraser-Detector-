{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Paraphrased Plagiarism Detection using Sentence Embeddings"
      ],
      "metadata": {
        "id": "xgVm1J8wCrC0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Goal: This notebook implements a system to detect potential paraphrased plagiarism in a student submission by comparing it against known source documents.\n",
        "\n",
        "Approach:\n",
        "1. Load Data: Read the submission text and source document texts.\n",
        "2. Preprocess: Segment texts into sentences.\n",
        "3. Embed Sentences: Use a pre-trained Sentence-BERT model (all-MiniLM-L6-v2)(may add more powerful bert models for selction) to convert sentences into meaningful numerical vectors (embeddings).\n",
        "4. Calculate Similarity:\n",
        "  * Cosine Similarity: Measure the semantic similarity between submission sentence embeddings and source sentence embeddings. High cosine similarity indicates similar meaning.\n",
        "  * Jaccard Similarity: Measure the lexical (word) overlap between sentence pairs. Low Jaccard similarity indicates different wording.\n",
        "5. Filter: Identify pairs with high semantic similarity (Cosine > threshold) but low lexical similarity (Jaccard < threshold) as potential candidates for paraphrased plagiarism.\n",
        "6. Evaluate: (Manual Step) Compare the flagged candidates against a known ground truth to assess performance."
      ],
      "metadata": {
        "id": "fDV-3rSRN1D9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup: Install Relevant Libraries and Mount Drive"
      ],
      "metadata": {
        "id": "cE8KNxESCxHv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we need to install the necessary libraries (`sentence-transformers` for the embedding model, `nltk` for text processing) and mount Google Drive to access our synethetic data files."
      ],
      "metadata": {
        "id": "tBybwypOC-df"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#install required libraries\n",
        "!pip install sentence-transformers nltk -q\n",
        "\n",
        "#import libraries\n",
        "import nltk\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "# Download necessary NLTK data for sentence tokenisation\n",
        "nltk.download('punkt', quiet=True) #advoid verbose output\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "\n",
        "# NLTK tokenizers\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "#Mount Google Drive\n",
        "try:\n",
        "  drive.mount('/content/drive')\n",
        "except Exception as e:\n",
        "  print(f\"Error mounting drive: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bh1rMzmrDVJ1",
        "outputId": "f7fd9ac0-3e84-4552-ad6b-19ec7c4e20a5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Load Data"
      ],
      "metadata": {
        "id": "tbphLbMnUzgZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defined paths to source and submission files on google drive (syenthetic dataset). Then, load the text content from these files"
      ],
      "metadata": {
        "id": "KZTRa65RU4zU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Define File Paths (UPDATE THESE WITH YOUR ACTUAL PATHS) ---\n",
        "source_photo_path = '/content/drive/MyDrive/NLP/Paraphrasing Detector/A3/Synethetic-Dataset/Sources/Source_Photosynthesis.txt'\n",
        "source_ml_path = '/content/drive/MyDrive/NLP/Paraphrasing Detector/A3/Synethetic-Dataset/Sources/Source_MachineLearning.txt'\n",
        "source_ww2_path = '/content/drive/MyDrive/NLP/Paraphrasing Detector/A3/Synethetic-Dataset/Sources/Source_WorldWar2.txt'\n",
        "submission_path = '/content/drive/MyDrive/NLP/Paraphrasing Detector/A3/Synethetic-Dataset/Submission/Submission1.txt'\n",
        "\n",
        "# --- Load Data ---\n",
        "\n",
        "with open(source_photo_path, 'r', encoding='utf-8') as f: # Added encoding='utf-8' for robustness\n",
        "    source1_photosynthesis = f.read()\n",
        "with open(source_ml_path, 'r', encoding='utf-8') as f:\n",
        "    source2_ML = f.read()\n",
        "with open(source_ww2_path, 'r', encoding='utf-8') as f:\n",
        "    source3_WW2 = f.read()\n",
        "with open(submission_path, 'r', encoding='utf-8') as f:\n",
        "    submission = f.read()\n",
        "\n",
        "print(\"Data loaded successfully:\")\n",
        "print(f\"- Photosynthesis source length: {len(source1_photosynthesis)} chars\")\n",
        "print(f\"- Machine Learning source length: {len(source2_ML)} chars\")\n",
        "print(f\"- World War 2 source length: {len(source3_WW2)} chars\")\n",
        "print(f\"- Submission length: {len(submission)} chars\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYkteKPvVKxj",
        "outputId": "78ea7509-592b-483b-9880-5128fee1809a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaded successfully:\n",
            "- Photosynthesis source length: 2339 chars\n",
            "- Machine Learning source length: 1989 chars\n",
            "- World War 2 source length: 2817 chars\n",
            "- Submission length: 3983 chars\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Data Preporcessing"
      ],
      "metadata": {
        "id": "kB5JNH3fVXgW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "tokenise each file into individual sentences. This allows us to compare sentences for the submission against sentences from the source"
      ],
      "metadata": {
        "id": "ORLGPd_VVbPU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine Sources and keep track of origin\n",
        "all_source_text = {\n",
        "    \"Photosynthesis\": source1_photosynthesis,\n",
        "    \"Machine Learning\": source2_ML,\n",
        "    \"World War 2\": source3_WW2\n",
        "}\n",
        "\n",
        "source_sentences_info = []\n",
        "min_sentence_length = 15 #filters out very short sentences as theyre irelevant\n",
        "\n",
        "print(\"Segmenting source documents into sentences\")\n",
        "for source_name, text in all_source_text.items():\n",
        "  sentences = sent_tokenize(text)\n",
        "\n",
        "  for i, sentence in enumerate(sentences):\n",
        "    # clean the sentence of whitespace\n",
        "    clean_sentence = re.sub(r'\\s+', ' ', sentence).strip()\n",
        "\n",
        "    if len(clean_sentence) >= min_sentence_length: #Advoid very short sentences\n",
        "      source_sentences_info.append(\n",
        "          {\n",
        "              \"text\": clean_sentence,\n",
        "              \"source\": source_name,\n",
        "              \"source_index\": i #represents original index position within its source document\n",
        "          }\n",
        "      )\n",
        "\n",
        "print(f\"Segmented source document into {len(source_sentences_info)} sentences (min length {min_sentence_length}).\")\n",
        "\n",
        "print(\"\\nSegmenting Submission document into sentences...\")\n",
        "submission_sentences = [\n",
        "    re.sub(r'\\s+', ' ', sentence).strip() for sentence in sent_tokenize(submission)\n",
        "    if len(re.sub(r'\\s+', ' ', sentence).strip()) >= min_sentence_length\n",
        "]\n",
        "\n",
        "print(f\"Segmented submission document into {len(submission_sentences)} sentences (min length {min_sentence_length}).\")\n",
        "\n",
        "# extract just the text for sentence embedding (needed for modelling)\n",
        "all_source_text = [sentence['text'] for sentence in source_sentences_info]\n",
        "\n",
        "# # Display first few sentences as a check\n",
        "# print(\"\\nFirst 3 source sentences:\")\n",
        "# for i in range(min(3, len(source_sentences_info))):\n",
        "#   print(f\"  [{source_sentences_info[i]['source']}]: {source_sentences_info[i]['text']}\")\n",
        "\n",
        "# print(\"\\nFirst 3 submission sentences:\")\n",
        "# for i in range(min(3, len(submission_sentences))):\n",
        "#     print(f\"  {submission_sentences[i]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tpvfrO-DWC0e",
        "outputId": "79813a00-68ad-4dc4-994f-7cea2482fae6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segmenting source documents into sentences\n",
            "Segmented source document into 42 sentences (min length 15).\n",
            "\n",
            "Segmenting Submission document into sentences...\n",
            "Segmented submission document into 28 sentences (min length 15).\n",
            "\n",
            "First 3 source sentences:\n",
            "  [Photosynthesis]: Photosynthesis (/ˌfoʊtəˈsɪnθəsɪs/ FOH-tə-SINTH-ə-sis) is a system of biological processes by which photosynthetic organisms, such as most plants, algae, and cyanobacteria, convert light energy, typically from sunlight, into the chemical energy necessary to fuel their metabolism.\n",
            "  [Photosynthesis]: Photosynthesis usually refers to oxygenic photosynthesis, a process that produces oxygen.\n",
            "  [Photosynthesis]: Photosynthetic organisms store the chemical energy so produced within intracellular organic compounds (compounds containing carbon) like sugars, glycogen, cellulose and starches.\n",
            "\n",
            "First 3 submission sentences:\n",
            "  Essay: Interconnected Systems - Nature, History, and Technology Our world is shaped by intricate processes, from the fundamental biological mechanisms sustaining life to the monumental historical events that alter global society, and now, by rapidly evolving technological capabilities that mimic learning itself.\n",
            "  Understanding these diverse systems offers insight into the complexity of change and adaptation across different scales.\n",
            "  These phenomena, though seemingly distinct, share underlying principles of interaction and evolution over time.\n"
          ]
        }
      ]
    }
  ]
}